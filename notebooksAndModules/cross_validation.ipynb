{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import an awesome Label helper\n",
      "import an awesome feature helper\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\razer\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\razer\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\razer\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\razer\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\razer\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\razer\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\razer\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import modules.labelHelper as lh \n",
    "import modules.featureHelper as fh\n",
    "from keras.callbacks import (ModelCheckpoint, LearningRateScheduler,\n",
    "                             EarlyStopping, ReduceLROnPlateau,CSVLogger)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.regularizers import l2 \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten,Conv1D, GlobalAveragePooling1D, MaxPooling1D,AveragePooling1D\n",
    "from keras.layers import Conv2D, MaxPooling2D,Conv1D,GlobalMaxPooling1D,MaxPooling1D,average, concatenate,RepeatVector,Lambda,add,subtract,Activation\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Input, Model\n",
    "from sklearn import metrics as mt\n",
    "from skimage.io import imshow\n",
    "from sklearn import preprocessing\n",
    "from matplotlib.patches import Rectangle\n",
    "from scipy.ndimage import maximum_filter1d\n",
    "from keras import backend as K\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "warnings.filterwarnings('ignore')\n",
    "from modules.modelTools import *\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from modules.plotHelper import *\n",
    "from matplotlib.patches import Rectangle\n",
    "from scipy.ndimage import maximum_filter1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 83 videos in total\n",
      "\n",
      "There are 19 trainees in total\n",
      "\n",
      " Number of Videos from each person\n",
      "Akhtar          -- [176 253 761] ----: 3\n",
      "Cadeddu         -- [164 171] ----: 2\n",
      "Crivelli        -- [225 276 333 425] ----: 4\n",
      "Gahan           -- [257 345 378 384 486] ----: 5\n",
      "Johnson         -- [441 615] ----: 2\n",
      "Keith           -- [240 277 717] ----: 3\n",
      "Kenigsberg      -- [ 78 440 455 471 881] ----: 5\n",
      "Krabbe          -- [152 344 527 861] ----: 4\n",
      "Marthur         -- [130 194 222 368 921] ----: 5\n",
      "Mollengarden    -- [ 59 143 267 460] ----: 4\n",
      "Moony           -- [294 301 361 498 539] ----: 5\n",
      "Passoni         -- [207 237 895 942] ----: 4\n",
      "Rozanski        -- [113 302 716] ----: 3\n",
      "Satyanarayan    -- [ 16  74 236 358 436 457 503 537 557 578 599 632 689] ----: 13\n",
      "Singla          -- [ 49 536 538] ----: 3\n",
      "Sorokin         -- [ 91 226 507 530] ----: 4\n",
      "Timburlake      -- [258 296 559 742] ----: 4\n",
      "Tse             -- [ 11 283 414 427 562 928] ----: 6\n",
      "Varun           -- [192 401 417 820] ----: 4\n"
     ]
    }
   ],
   "source": [
    "ll=lh.Label('../2019_fall_labels.csv')\n",
    "print(\"There are %d videos in total\\n\"%ll.video_count())\n",
    "ll.get_trainee_info()\n",
    "\n",
    "train_fold,test_fold=ll.cross_validation_on_people(5)\n",
    "# print(train_fold)\n",
    "# print(test_fold)\n",
    "\n",
    "## set up feature parameters ##\n",
    "folder = \"../2019_fall_video_features\"\n",
    "video_clips_length=30\n",
    "time_lag=2\n",
    "move_threshold=150\n",
    "stride=video_clips_length\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In fold 1\n",
      "\n",
      "(2237, 30, 28)\n",
      "(2237, 6)\n",
      "(465, 30, 28)\n",
      "(465, 6)\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.18708, saving model to model_zoo/MTmodel1.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.18708 to 1.79067, saving model to model_zoo/MTmodel1.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.79067 to 1.59773, saving model to model_zoo/MTmodel1.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.59773 to 1.45946, saving model to model_zoo/MTmodel1.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.45946 to 1.39594, saving model to model_zoo/MTmodel1.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.39594 to 1.18321, saving model to model_zoo/MTmodel1.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.18321 to 1.04183, saving model to model_zoo/MTmodel1.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.04183 to 0.78117, saving model to model_zoo/MTmodel1.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.78117 to 0.75817, saving model to model_zoo/MTmodel1.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.75817 to 0.66247, saving model to model_zoo/MTmodel1.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.66247 to 0.59520, saving model to model_zoo/MTmodel1.h5\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.59520 to 0.59230, saving model to model_zoo/MTmodel1.h5\n",
      "In fold 2\n",
      "\n",
      "(2229, 30, 28)\n",
      "(2229, 6)\n",
      "(473, 30, 28)\n",
      "(473, 6)\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.80859, saving model to model_zoo/MTmodel2.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.80859 to 1.66845, saving model to model_zoo/MTmodel2.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.66845 to 1.22635, saving model to model_zoo/MTmodel2.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.22635 to 0.85935, saving model to model_zoo/MTmodel2.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.85935 to 0.78428, saving model to model_zoo/MTmodel2.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.78428 to 0.58826, saving model to model_zoo/MTmodel2.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.58826 to 0.47700, saving model to model_zoo/MTmodel2.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.47700 to 0.42121, saving model to model_zoo/MTmodel2.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.42121\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.42121 to 0.41962, saving model to model_zoo/MTmodel2.h5\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.41962\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.41962\n",
      "In fold 3\n",
      "\n",
      "(2206, 30, 28)\n",
      "(2206, 6)\n",
      "(496, 30, 28)\n",
      "(496, 6)\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.27246, saving model to model_zoo/MTmodel3.h5\n"
     ]
    }
   ],
   "source": [
    "per_gear_score_folds=[]\n",
    "filtered_per_gear_score_folds=[]\n",
    "\n",
    "predict_label_folds=[]\n",
    "filtered_predict_label_folds=[]\n",
    "gt_label_folds=[]\n",
    "\n",
    "acc=[]\n",
    "filtered_acc=[]\n",
    "\n",
    "it=1\n",
    "for train_video_number,test_video_number in zip(train_fold,test_fold):\n",
    "    print(\"In fold %d\\n\" %it)\n",
    "    \n",
    "    predict_label=[]\n",
    "    filtered_predict_label=[]\n",
    "    gt_label=[]\n",
    "    # -----------in each fold ---------------------------------------------------\n",
    "    # train test split by people\n",
    "    train_video,train_video_label=fh.make_train_test_data_from_video_numbers(folder,train_video_number,video_clips_length,time_lag,move_threshold,stride,ll)\n",
    "    test_video,test_video_label=fh.make_train_test_data_from_video_numbers(folder,test_video_number,video_clips_length,time_lag,move_threshold,stride,ll)\n",
    "    print(train_video.shape)\n",
    "    print(train_video_label.shape)\n",
    "    print(test_video.shape)\n",
    "    print(test_video_label.shape)\n",
    "    \n",
    "    # -----------pre-processing  ---------------------------------------------------\n",
    "    X_train=train_video/640-0.5\n",
    "    X_test=test_video/640-0.5\n",
    "    y_train=train_video_label/5\n",
    "    y_test=test_video_label/5\n",
    "\n",
    "    # -----------init data generator  ---------------------------------------------------\n",
    "    sample_of_trainningdata=X_train.shape[0]\n",
    "    sample_of_testdata=X_test.shape[0]\n",
    "    batch_size=64\n",
    "    train_gen=generator(X_train, y_train, batch_size,0.1)\n",
    "    val_gen=generator(X_test, y_test, batch_size,0)\n",
    "\n",
    "    # -----------#prepare for model in&out shape ---------------------------------------------------\n",
    "    clip_lenth=X_train.shape[1]\n",
    "    dimension=X_train.shape[2]\n",
    "    l2_lambda=0.01\n",
    "    model=make_model(0.01,clip_lenth,dimension)\n",
    "\n",
    "    callbacks_list = get_callback_list_by_model('MTmodel'+str(it))\n",
    "    \n",
    "    model_h=model.fit_generator(train_gen,\n",
    "                        steps_per_epoch=sample_of_trainningdata//batch_size,\n",
    "                        epochs=12,\n",
    "                        validation_data=val_gen,\n",
    "                       validation_steps=sample_of_testdata//batch_size,\n",
    "                        verbose=0,\n",
    "                        callbacks=callbacks_list\n",
    "     )\n",
    "    \n",
    "    # -----------#plot training results ---------------------------------------------------\n",
    "    ax=plt.figure(figsize=(12,14))\n",
    "    ax = plt.subplot(421)\n",
    "    ax.plot(model_h.history['loss'])\n",
    "    plt.ylabel('Training Loss')\n",
    "    plt.xlabel('epochs')\n",
    "\n",
    "    ax = plt.subplot(422)\n",
    "    ax.plot(model_h.history['val_loss'])\n",
    "    plt.ylabel('Validation Loss')\n",
    "    plt.xlabel('epochs')\n",
    "    model.load_weights('model_zoo/MTmodel{}.h5'.format(str(it)))\n",
    "    \n",
    "    # -----------calcuate pre gear score acc and category prediction---------------------------------------------------\n",
    "    valid_arr=np.zeros(6)\n",
    "    filtered_valid_arr=np.zeros(6)\n",
    "    for t_number in test_video_number:\n",
    "        re,gt = model_predit_by_videoNumber(model,folder,t_number,video_clips_length,time_lag,move_threshold,stride,ll) \n",
    "        \n",
    "        #-------------max filter data\n",
    "        filtered_data=pd.DataFrame(re,columns=['DP','BD','E','FS','A','RC'])\n",
    "        for index, row in filtered_data.iteritems():\n",
    "            filtered_data[index]=maximum_filter1d(filtered_data[index],mode='nearest',size= 3)\n",
    "        #-------------max filter data\n",
    "#         print(re)\n",
    "#         print(filtered_data)\n",
    "        #-------------category prediction\n",
    "        predict_level,predict_totalScore=convert_to_skill(re,'mean')\n",
    "        filter_predict_level,filter_predict_totalScore=convert_to_skill(filtered_data,'mean')\n",
    "        gt_level,gt_totalScore=ll.get_video_level_by_video_number(t_number,'mean')\n",
    "\n",
    "#         print(\"Predict vs GT level:\",predict_level,gt_level)\n",
    "#         print(\"Predict Score vs filter_predict_level Score:\",predict_totalScore,filter_predict_totalScore)\n",
    "        predict_label.append(predict_level)\n",
    "        filtered_predict_label.append(filter_predict_level)\n",
    "        gt_label.append(gt_level)\n",
    "        \n",
    "        #-------------category prediction\n",
    "        \n",
    "        #-------------pre gear score acc \n",
    "        filtered_re=np.round(np.mean(filtered_data,axis=0))\n",
    "        re=np.round(np.mean(re,axis=0))\n",
    "        \n",
    "        score_range=ll.get_video_score_range_by_video_number(t_number)\n",
    "        maxScore=score_range[0]\n",
    "        minScore=score_range[1]\n",
    "        for index,val in enumerate(['DP','BD','E','FS','A','RC']):\n",
    "            if re[index]>=int(minScore[val]) and re[index]<=int(maxScore[val]):\n",
    "                valid_arr[index]+=1\n",
    "            if filtered_re[index]>=int(minScore[val]) and filtered_re[index]<=int(maxScore[val]):\n",
    "                filtered_valid_arr[index]+=1\n",
    "#             print(re[index], filtered_valid_arr[index])\n",
    "        #-------------pre gear score acc \n",
    "        \n",
    "    predict_label_folds.append(predict_label)\n",
    "    filtered_predict_label_folds.append(filtered_predict_label)\n",
    "    gt_label_folds.append(gt_label)  \n",
    "    acc.append(sum(np.array(predict_label)==np.array(gt_label))/len(predict_label))\n",
    "    filtered_acc.append(sum(np.array(filtered_predict_label)==np.array(gt_label))/len(predict_label))\n",
    "    \n",
    "    per_gear_score_folds.append(valid_arr/len(test_video_number))\n",
    "    filtered_per_gear_score_folds.append(filtered_valid_arr/len(test_video_number))\n",
    "    it+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(per_gear_score_folds)\n",
    "print(filtered_per_gear_score_folds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_label_folds)\n",
    "print(filtered_predict_label_folds)\n",
    "print(gt_label_folds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(acc)\n",
    "print(filtered_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
