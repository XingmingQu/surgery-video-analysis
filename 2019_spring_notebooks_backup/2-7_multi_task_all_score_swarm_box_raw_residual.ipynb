{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\razer\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "import os\n",
    "from os.path import join as pjoin\n",
    "from matplotlib import pyplot as plt\n",
    "from keras import backend as K\n",
    "import random\n",
    "import warnings\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.regularizers import l2 \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D,Conv1D,GlobalMaxPooling1D,MaxPooling1D\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D,AveragePooling1D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import average \n",
    "from keras.models import Input, Model\n",
    "from sklearn import metrics as mt\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage.io import imshow\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib.patches import Rectangle\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# first we can see what does the feature look like\n",
    "def get_raw_feature():\n",
    "    video_data_dir='new23feature'\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    for video in os.listdir(video_data_dir):\n",
    "        each_video= pjoin(video_data_dir,video)\n",
    "        ## 'C:\\\\videofeature\\\\' using\n",
    "#         l=each_video.split('t_')[1]\n",
    "\n",
    "        # 'C:\\\\new_video_feature\\\\' using\n",
    "        l=each_video.split('e_')[1]\n",
    "        l=l.split('.')[0]\n",
    "        LABEL=int(l)\n",
    "        X.append(np.load(each_video))\n",
    "        Y.append(LABEL)\n",
    "#     Video_label=pd.read_csv('label.csv')\n",
    "# #     Video_label=np.array(Video_label)[:,:-1]\n",
    "    return X,Y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Read features from 19 npy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are  23 videos\n",
      "feature of video [0] (153, 14)\n",
      "video [0] is from video 11\n",
      "[11, 113, 130, 143, 152, 16, 164, 171, 176, 192, 194, 207, 236, 237, 240, 253, 257, 258, 49, 59, 74, 78, 91]\n"
     ]
    }
   ],
   "source": [
    "X,Y=get_raw_feature()\n",
    "Video_label=pd.read_csv('newall_label.csv')\n",
    "# Video_label=np.array(Video_label)[:,:-1]\n",
    "print('there are ',len(X),'videos')\n",
    "print('feature of video [0]',X[0].shape)\n",
    "print('video [0] is from video' ,Y[0])\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Video_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VideoNum</th>\n",
       "      <th>DP</th>\n",
       "      <th>BD</th>\n",
       "      <th>E</th>\n",
       "      <th>FS</th>\n",
       "      <th>A</th>\n",
       "      <th>RC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VideoNum  DP  BD  E  FS  A  RC\n",
       "0        11   3   2  2   2  4   3\n",
       "1        11   2   2  3   1  3   3\n",
       "2        11   1   1  1   1  1   1\n",
       "3        16   1   1  1   1  1   1\n",
       "4        16   2   1  1   1  1   2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range_label=pd.read_csv('range_label.csv')\n",
    "range_label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts=range_label['VideoNum'].value_counts()\n",
    "# counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_review_times(videoNum):\n",
    "    counts=range_label['VideoNum'].value_counts()\n",
    "    times=int(counts[counts.index==videoNum])\n",
    "    return times\n",
    "get_review_times(192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_group=range_label.groupby(['VideoNum'])\n",
    "r_max=r_group.max()\n",
    "r_min=r_group.min()\n",
    "r_max=r_max.reset_index()\n",
    "r_min=r_min.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_video_range(videoNum):\n",
    "    vmax=r_max[r_max['VideoNum']==videoNum]\n",
    "    vmin=r_min[r_min['VideoNum']==videoNum]\n",
    "    return vmax,vmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a,i=get_video_range(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video_label contains detail labels and we can use Y to index them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def getlabel(number):\n",
    "    L=Video_label[Video_label['label']==number]\n",
    "    L=np.array(L)[:,:-1]\n",
    "    L= np.squeeze(L)\n",
    "    return L\n",
    "# def getlabel_addnoise(number):\n",
    "#     L=Video_label[Video_label['label']==number]\n",
    "#     L=np.array(L)[:,:-1]\n",
    "#     L= np.squeeze(L)\n",
    "#     noise=np.array([random.uniform(-0.4, 0.4) for _ in range(len(L))])\n",
    "# #     print(noise)\n",
    "#     return L+noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create two functions to sample video clips from each video and get their time differences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0,
     13
    ]
   },
   "outputs": [],
   "source": [
    "def get_diff_and_hstack_to_orginal_data(X,time_lag=2,move_threshold=200):\n",
    "    original=X[:-time_lag]\n",
    "    modified=X[time_lag:]\n",
    "    result=modified-original\n",
    "    ## threshold\n",
    "    # consider there was no top in the first image and it showed up in the next image\n",
    "    # the difference would be huge, which was not ideal.\n",
    "    # so we need to filter these extrem value \n",
    "    result[np.abs(result)>move_threshold]=0\n",
    "    \n",
    "    return np.hstack((X[time_lag:],result))\n",
    "\n",
    "##from each video sample video clips with size=window_L. you can specify stride \n",
    "def make_video_clips(matrix,window_L,stride):\n",
    "    alldata=[]\n",
    "\n",
    "    total_frame=matrix.shape[0]\n",
    "    index=[n for n in range(1,total_frame,stride)]\n",
    "    for start_index in index:\n",
    "        if start_index+window_L> total_frame:\n",
    "            break\n",
    "#         print(start_index)\n",
    "        each_clip_data=matrix[start_index:start_index+window_L]\n",
    "#         each_clip_data=np.transpose(each_clip_data)\n",
    "#         print(each_clip_data.shape)\n",
    "        alldata.append(each_clip_data)\n",
    "    return np.array(alldata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 pre-processing the feature\n",
    "make new 28-d feature and  sample video clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 30, 28)\n",
      "(4, 30, 28)\n",
      "(10, 30, 28)\n",
      "(20, 30, 28)\n",
      "(10, 30, 28)\n",
      "(100, 30, 28)\n",
      "(40, 30, 28)\n",
      "(19, 30, 28)\n",
      "(100, 30, 28)\n",
      "(5, 30, 28)\n",
      "(100, 30, 28)\n",
      "(10, 30, 28)\n",
      "(10, 30, 28)\n",
      "(5, 30, 28)\n",
      "(20, 30, 28)\n",
      "(40, 30, 28)\n",
      "(99, 30, 28)\n",
      "(9, 30, 28)\n",
      "(20, 30, 28)\n",
      "(40, 30, 28)\n",
      "(20, 30, 28)\n",
      "(10, 30, 28)\n",
      "(10, 30, 28)\n"
     ]
    }
   ],
   "source": [
    "####################################### set video clips parameters\n",
    "############################################### make new 28-d feature\n",
    "video_clips_length=30\n",
    "time_lag=2\n",
    "move_threshold=150\n",
    "stride=video_clips_length\n",
    "\n",
    "####################################### set video clips parameters\n",
    "############################################### make new 28-d feature\n",
    "all_data=[]\n",
    "for each_video,label in zip(X,Y):\n",
    "\n",
    "    re=get_diff_and_hstack_to_orginal_data(each_video,time_lag,move_threshold)\n",
    "#     re=each_video\n",
    "    video_clip=make_video_clips(re,video_clips_length,stride)\n",
    "    print(video_clip.shape)\n",
    "    all_data.append((video_clip))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are 19 videos and you can see how many video clips they have.\n",
    "\n",
    "And next we will use leave one method to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 prepare leave one data and give every video clips their label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# this function is used to stack 18 videos into a big matrix\n",
    "# the dimenstion will be \n",
    "# (n1+n2..+n18,28,30)\n",
    "# and the label will be (n1+n2..+n18,6)\n",
    "\n",
    "def stack_video_clips_get_label(data,label):\n",
    "    final_data=data[0]\n",
    "\n",
    "    final_label=[getlabel(label[0]) for _ in range(final_data.shape[0])]\n",
    "#     print(len(final_label))\n",
    "#     print(final_data.shape)\n",
    "    for i in range(1,len(data)):\n",
    "        final_data=np.vstack((final_data,data[i]))\n",
    "        for j in range(data[i].shape[0]):\n",
    "            final_label.append(getlabel(label[i]))\n",
    "    return final_data,np.array(final_label)\n",
    "\n",
    "########## leave one and stack all video clips\n",
    "def hold_out(X,Y,hold_number):\n",
    "        new_label=Y.copy()\n",
    "        new_data=X.copy()\n",
    "        # find video n 's index\n",
    "        index=new_label.index(hold_number)\n",
    "        # get the video n and it's label\n",
    "        X_test=new_data[index]\n",
    "        test_label=getlabel(hold_number)\n",
    "        y_test=np.array([test_label for _ in range(X_test.shape[0])])\n",
    "#         # so we can del them \n",
    "        del new_label[index]\n",
    "        del new_data[index]\n",
    "#         print(new_label)\n",
    "        final_data_X,final_data_Y=stack_video_clips_get_label(new_data,new_label)\n",
    "        return final_data_X,X_test,final_data_Y,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# for each col, calculate the weighted average\n",
    "def get_weighted_average(df):\n",
    "    length=len(df)\n",
    "    top=int(1/6*length)\n",
    "    bottom=length-top\n",
    "    df=np.array(df)\n",
    "    df=np.sort(df)\n",
    "    weghted_sum=np.sum(df[:top]*(1/6))+np.sum(df[top+1:bottom]*(2/3))+np.sum(df[bottom+1:]*(1/6))\n",
    "    weghted_sum=weghted_sum/length\n",
    "    return weghted_sum\n",
    "\n",
    "# make the data frame\n",
    "# Plot_data is the pridect result, which will have 6 scores\n",
    "def get_video_score(Plot_data,GT,poolingweight):\n",
    "    weight_ave_re=[]\n",
    "    mean_re=[]\n",
    "    median_re=[]\n",
    "    pooled_re=[]\n",
    "    \n",
    "    for i in Plot_data.columns:\n",
    "        weight_ave=get_weighted_average(Plot_data[i])\n",
    "        weight_ave_re.append(weight_ave)\n",
    "        \n",
    "        mean_value=np.mean(Plot_data[i])   \n",
    "        mean_re.append(mean_value)\n",
    "        \n",
    "        median_value = np.median(Plot_data[i])\n",
    "        median_re.append(median_value)\n",
    "        \n",
    "        weight_ave_wight=poolingweight[0]\n",
    "        mean_value_wight=poolingweight[1]\n",
    "        median_value_wight=poolingweight[2]\n",
    "        # pooling is also a weighted average \n",
    "        pooling = (weight_ave*weight_ave_wight + mean_value*mean_value_wight + median_value*median_value_wight)\n",
    "        pooled_re.append(pooling)\n",
    "        \n",
    "    video_result=pd.DataFrame([weight_ave_re,mean_re,median_re,pooled_re,GT])\n",
    "    video_result.columns = ['DP','BD','E','FS','A','RC']\n",
    "    video_result.index = ['Weighted_average', 'Mean', 'Median','Cool_pooling','Ground_truth']\n",
    "    return video_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 perform leave one cross validation  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import average, concatenate,RepeatVector,Lambda,add,subtract,Activation\n",
    "from keras.models import Input, Model\n",
    "from keras import backend as K\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# the model we will use\n",
    "def make_model_1d_3layer(l2_lambda,clip_lenth,dimension):\n",
    "    input_holder = Input(shape=(clip_lenth, dimension))\n",
    "    x = Conv1D(filters=8, \n",
    "                     kernel_size=15, \n",
    "                     padding='same',\n",
    "                     activation='relu', \n",
    "                     input_shape=(clip_lenth, dimension),\n",
    "                     kernel_regularizer=l2(l2_lambda)\n",
    "              )(input_holder)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Conv1D(filters=8, \n",
    "                     kernel_size=10, \n",
    "                     padding='same',\n",
    "                     activation='relu', \n",
    "                     input_shape=(clip_lenth, dimension),\n",
    "                    kernel_regularizer=l2(l2_lambda)\n",
    "              )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv1D(filters=8, \n",
    "                     kernel_size=10, \n",
    "                     padding='same',\n",
    "                     activation='relu', \n",
    "                     input_shape=(clip_lenth, dimension),\n",
    "                     kernel_regularizer=l2(l2_lambda))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x= MaxPooling1D(2,padding='same')(x)\n",
    "    \n",
    "    x = Conv1D(filters=16, \n",
    "                     kernel_size=5, \n",
    "                     padding='same',\n",
    "                     activation='relu', \n",
    "                     input_shape=(clip_lenth, dimension),\n",
    "                    kernel_regularizer=l2(l2_lambda)\n",
    "              )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Conv1D(filters=16, \n",
    "                     kernel_size=5, \n",
    "                     padding='same',\n",
    "                     activation='relu', \n",
    "                     input_shape=(clip_lenth, dimension),\n",
    "                     kernel_regularizer=l2(l2_lambda)\n",
    "              )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv1D(filters=16, \n",
    "                     kernel_size=5, \n",
    "                     padding='same',\n",
    "                     activation='relu', \n",
    "                     input_shape=(clip_lenth, dimension),\n",
    "                     kernel_regularizer=l2(l2_lambda))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "#     x = GlobalMaxPooling1D()(x)\n",
    "#     x = Flatten()(x)\n",
    "\n",
    "#####_______________________________________________________________________\n",
    "    u = GlobalMaxPooling1D()(x)\n",
    "    u_broadcast=RepeatVector(x.shape[1])(u)\n",
    "    \n",
    "    def op(inputs):\n",
    "        x, y = inputs\n",
    "        return K.pow((x - y), 2) \n",
    "\n",
    "    Z=Lambda(op)([u_broadcast,x])\n",
    "\n",
    "    v = GlobalMaxPooling1D()(Z)\n",
    "    x = concatenate([u,v])\n",
    "    \n",
    "#####_______________________________________________________________________\n",
    "#     x = Dense(16, \n",
    "#               activation='relu',\n",
    "#               kernel_initializer='he_uniform',\n",
    "#               kernel_regularizer=l2(l2_lambda)\n",
    "#             )(x)\n",
    "\n",
    "#     x = Dropout(0.5)(x) # add some dropout for regularization after conv layers\n",
    "    \n",
    "#     x = Dense(6, \n",
    "#               activation='sigmoid', \n",
    "#               kernel_initializer='glorot_uniform',\n",
    "#               kernel_regularizer=l2(l2_lambda)\n",
    "#              )(x)\n",
    "    \n",
    "#####_______________________________________________________________________  \n",
    "#     x = Dense(16,activation='relu',\n",
    "#               kernel_regularizer=l2(l2_lambda)\n",
    "#              )(x)\n",
    "#     x = Dropout(0.5)(x) # add some dropout for regularization after conv layers\n",
    "\n",
    "\n",
    "#     x = Dense(32,activation='relu',kernel_initializer='he_uniform',\n",
    "#               kernel_regularizer=l2(l2_lambda)\n",
    "#              )(x)\n",
    "#     x = Dropout(0.5)(x) # add some dropout for regularization after conv layers\n",
    "\n",
    "#####______________________________Multi task_________________________________________  \n",
    "# ['DP','BD','E','FS','A','RC']\n",
    "    number_of_N=16\n",
    "    DP=0.2\n",
    "    \n",
    "    y1 = Dense(number_of_N,kernel_initializer='he_uniform',\n",
    "             # kernel_regularizer=l2(l2_lambda)\n",
    "             )(x)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "    y1 = Activation('relu')(y1)\n",
    "    y1 = Dropout(DP)(y1) # add some dropout for regularization after conv layers\n",
    "    y1= Dense(1, activation='sigmoid',kernel_initializer='glorot_uniform',name='DP'\n",
    "#              kernel_regularizer=l2(l2_lambda)\n",
    "             )(y1)\n",
    "\n",
    "    y2 = Dense(number_of_N,kernel_initializer='he_uniform',\n",
    "             # kernel_regularizer=l2(l2_lambda)\n",
    "             )(x)\n",
    "    y2 = BatchNormalization()(y2)\n",
    "    y2 = Activation('relu')(y2)\n",
    "    y2 = Dropout(DP)(y2) # add some dropout for regularization after conv layers\n",
    "    y2= Dense(1, activation='sigmoid',kernel_initializer='glorot_uniform',name='BD'\n",
    "#              kernel_regularizer=l2(l2_lambda)\n",
    "             )(y2)\n",
    "    \n",
    "    y3 = Dense(number_of_N,kernel_initializer='he_uniform',\n",
    "             # kernel_regularizer=l2(l2_lambda)\n",
    "             )(x)\n",
    "    y3 = BatchNormalization()(y3)\n",
    "    y3 = Activation('relu')(y3)\n",
    "    y3 = Dropout(DP)(y3) # add some dropout for regularization after conv layers\n",
    "    y3= Dense(1, activation='sigmoid',kernel_initializer='glorot_uniform',name='E'\n",
    "#              kernel_regularizer=l2(l2_lambda)\n",
    "             )(y3)\n",
    "\n",
    "    y4 = Dense(number_of_N,kernel_initializer='he_uniform',\n",
    "             # kernel_regularizer=l2(l2_lambda)\n",
    "             )(x)\n",
    "    y4 = BatchNormalization()(y4)\n",
    "    y4 = Activation('relu')(y4)\n",
    "    y4 = Dropout(DP)(y4) # add some dropout for regularization after conv layers\n",
    "    y4= Dense(1, activation='sigmoid',kernel_initializer='glorot_uniform',name='FS'\n",
    "#              kernel_regularizer=l2(l2_lambda)\n",
    "             )(y4)\n",
    "    \n",
    "    y5 = Dense(number_of_N,kernel_initializer='he_uniform',\n",
    "             # kernel_regularizer=l2(l2_lambda)\n",
    "             )(x)\n",
    "    y5 = BatchNormalization()(y5)\n",
    "    y5 = Activation('relu')(y5)\n",
    "    y5 = Dropout(DP)(y5) # add some dropout for regularization after conv layers\n",
    "    y5= Dense(1, activation='sigmoid',kernel_initializer='glorot_uniform',name='A'\n",
    "#              kernel_regularizer=l2(l2_lambda)\n",
    "             )(y5)\n",
    "    \n",
    "    y6 = Dense(number_of_N,kernel_initializer='he_uniform',\n",
    "             # kernel_regularizer=l2(l2_lambda)\n",
    "             )(x)\n",
    "    y6 = BatchNormalization()(y6)\n",
    "    y6 = Activation('relu')(y6)\n",
    "    y6 = Dropout(DP)(y6) # add some dropout for regularization after conv layers\n",
    "    y6= Dense(1, activation='sigmoid',kernel_initializer='glorot_uniform',name='RC'\n",
    "#              kernel_regularizer=l2(l2_lambda)\n",
    "             )(y6)\n",
    "#####______________________________Multi task_________________________________________  \n",
    "\n",
    "\n",
    "    model = Model(inputs=input_holder,outputs=[y1,y2,y3,y4,y5,y6])\n",
    "# ['DP','BD','E','FS','A','RC']\n",
    "    losses = {\n",
    "        \"DP\": \"mean_squared_error\",\n",
    "        \"BD\": \"mean_squared_error\",\n",
    "        \"E\": \"mean_squared_error\",\n",
    "        \"FS\": \"mean_squared_error\",\n",
    "        \"A\": \"mean_squared_error\",\n",
    "        \"RC\": \"mean_squared_error\",\n",
    "    }\n",
    "    model.compile(#loss='mean_squared_error', # 'categorical_crossentropy' 'mean_squared_error' 'mean_absolute_percentage_error'\n",
    "                  loss=losses, \n",
    "              optimizer='adam') # 'adadelta' 'rmsprop'                  \n",
    "#     model.summary()\n",
    "    return model\n",
    "te=make_model_1d_3layer(0.001,30,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# the model we will use\n",
    "def make_model_1d(l2_lambda,clip_lenth,dimension):\n",
    "    input_holder = Input(shape=(clip_lenth, dimension))\n",
    "    x = Conv1D(filters=8, \n",
    "                     kernel_size=15, \n",
    "                     padding='same',\n",
    "                     activation='relu', \n",
    "                     input_shape=(clip_lenth, dimension),\n",
    "                     kernel_regularizer=l2(l2_lambda)\n",
    "              )(input_holder)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Conv1D(filters=8, \n",
    "                     kernel_size=10, \n",
    "                     padding='same',\n",
    "                     activation='relu', \n",
    "                     input_shape=(clip_lenth, dimension),\n",
    "                    kernel_regularizer=l2(l2_lambda)\n",
    "              )(x)\n",
    "#     x = Conv1D(filters=8, \n",
    "#                      kernel_size=5, \n",
    "#                      padding='same',\n",
    "#                      activation='relu', \n",
    "#                      input_shape=(clip_lenth, dimension),\n",
    "#                      kernel_regularizer=l2(l2_lambda))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x= MaxPooling1D(2,padding='same')(x)\n",
    "    \n",
    "    x = Conv1D(filters=16, \n",
    "                     kernel_size=5, \n",
    "                     padding='same',\n",
    "                     activation='relu', \n",
    "                     input_shape=(clip_lenth, dimension),\n",
    "                    kernel_regularizer=l2(l2_lambda)\n",
    "              )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Conv1D(filters=16, \n",
    "                     kernel_size=5, \n",
    "                     padding='same',\n",
    "                     activation='relu', \n",
    "                     input_shape=(clip_lenth, dimension),\n",
    "                     kernel_regularizer=l2(l2_lambda)\n",
    "              )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "#     x = Conv1D(filters=16, \n",
    "#                      kernel_size=5, \n",
    "#                      padding='same',\n",
    "#                      activation='relu', \n",
    "#                      input_shape=(clip_lenth, dimension),\n",
    "#                      kernel_regularizer=l2(l2_lambda))(x)\n",
    "\n",
    "#     x = GlobalMaxPooling1D()(x)\n",
    "#     x = Flatten()(x)\n",
    "\n",
    "#####_______________________________________________________________________\n",
    "    u = GlobalMaxPooling1D()(x)\n",
    "    u_broadcast=RepeatVector(x.shape[1])(u)\n",
    "    \n",
    "    def op(inputs):\n",
    "        x, y = inputs\n",
    "        return K.pow((x - y), 2) \n",
    "\n",
    "    Z=Lambda(op)([u_broadcast,x])\n",
    "\n",
    "    v = GlobalMaxPooling1D()(Z)\n",
    "    x = concatenate([u,v])\n",
    "    \n",
    "#####_______________________________________________________________________\n",
    "#     x = Dense(16, \n",
    "#               activation='relu',\n",
    "#               kernel_initializer='he_uniform',\n",
    "#               kernel_regularizer=l2(l2_lambda)\n",
    "#             )(x)\n",
    "\n",
    "#     x = Dropout(0.5)(x) # add some dropout for regularization after conv layers\n",
    "    \n",
    "#     x = Dense(6, \n",
    "#               activation='sigmoid', \n",
    "#               kernel_initializer='glorot_uniform',\n",
    "#               kernel_regularizer=l2(l2_lambda)\n",
    "#              )(x)\n",
    "    \n",
    "#####_______________________________________________________________________  \n",
    "#     x = Dense(16,activation='relu',\n",
    "#               kernel_regularizer=l2(l2_lambda)\n",
    "#              )(x)\n",
    "#     x = Dropout(0.5)(x) # add some dropout for regularization after conv layers\n",
    "\n",
    "\n",
    "#     x = Dense(32,activation='relu',kernel_initializer='he_uniform',\n",
    "#               kernel_regularizer=l2(l2_lambda)\n",
    "#              )(x)\n",
    "#     x = Dropout(0.5)(x) # add some dropout for regularization after conv layers\n",
    "\n",
    "#####______________________________Multi task_________________________________________  \n",
    "# ['DP','BD','E','FS','A','RC']\n",
    "    number_of_N=16\n",
    "    DP=0.2\n",
    "    \n",
    "    y1 = Dense(number_of_N,kernel_initializer='he_uniform',\n",
    "             # kernel_regularizer=l2(l2_lambda)\n",
    "             )(x)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "    y1 = Activation('relu')(y1)\n",
    "    y1 = Dropout(DP)(y1) # add some dropout for regularization after conv layers\n",
    "    y1= Dense(1, activation='sigmoid',kernel_initializer='glorot_uniform',name='DP'\n",
    "#              kernel_regularizer=l2(l2_lambda)\n",
    "             )(y1)\n",
    "\n",
    "    y2 = Dense(number_of_N,kernel_initializer='he_uniform',\n",
    "             # kernel_regularizer=l2(l2_lambda)\n",
    "             )(x)\n",
    "    y2 = BatchNormalization()(y2)\n",
    "    y2 = Activation('relu')(y2)\n",
    "    y2 = Dropout(DP)(y2) # add some dropout for regularization after conv layers\n",
    "    y2= Dense(1, activation='sigmoid',kernel_initializer='glorot_uniform',name='BD'\n",
    "#              kernel_regularizer=l2(l2_lambda)\n",
    "             )(y2)\n",
    "    \n",
    "    y3 = Dense(number_of_N,kernel_initializer='he_uniform',\n",
    "             # kernel_regularizer=l2(l2_lambda)\n",
    "             )(x)\n",
    "    y3 = BatchNormalization()(y3)\n",
    "    y3 = Activation('relu')(y3)\n",
    "    y3 = Dropout(DP)(y3) # add some dropout for regularization after conv layers\n",
    "    y3= Dense(1, activation='sigmoid',kernel_initializer='glorot_uniform',name='E'\n",
    "#              kernel_regularizer=l2(l2_lambda)\n",
    "             )(y3)\n",
    "\n",
    "    y4 = Dense(number_of_N,kernel_initializer='he_uniform',\n",
    "             # kernel_regularizer=l2(l2_lambda)\n",
    "             )(x)\n",
    "    y4 = BatchNormalization()(y4)\n",
    "    y4 = Activation('relu')(y4)\n",
    "    y4 = Dropout(DP)(y4) # add some dropout for regularization after conv layers\n",
    "    y4= Dense(1, activation='sigmoid',kernel_initializer='glorot_uniform',name='FS'\n",
    "#              kernel_regularizer=l2(l2_lambda)\n",
    "             )(y4)\n",
    "    \n",
    "    y5 = Dense(number_of_N,kernel_initializer='he_uniform',\n",
    "             # kernel_regularizer=l2(l2_lambda)\n",
    "             )(x)\n",
    "    y5 = BatchNormalization()(y5)\n",
    "    y5 = Activation('relu')(y5)\n",
    "    y5 = Dropout(DP)(y5) # add some dropout for regularization after conv layers\n",
    "    y5= Dense(1, activation='sigmoid',kernel_initializer='glorot_uniform',name='A'\n",
    "#              kernel_regularizer=l2(l2_lambda)\n",
    "             )(y5)\n",
    "    \n",
    "    y6 = Dense(number_of_N,kernel_initializer='he_uniform',\n",
    "             # kernel_regularizer=l2(l2_lambda)\n",
    "             )(x)\n",
    "    y6 = BatchNormalization()(y6)\n",
    "    y6 = Activation('relu')(y6)\n",
    "    y6 = Dropout(DP)(y6) # add some dropout for regularization after conv layers\n",
    "    y6= Dense(1, activation='sigmoid',kernel_initializer='glorot_uniform',name='RC'\n",
    "#              kernel_regularizer=l2(l2_lambda)\n",
    "             )(y6)\n",
    "#####______________________________Multi task_________________________________________  \n",
    "\n",
    "\n",
    "    model = Model(inputs=input_holder,outputs=[y1,y2,y3,y4,y5,y6])\n",
    "# ['DP','BD','E','FS','A','RC']\n",
    "    losses = {\n",
    "        \"DP\": \"mean_squared_error\",\n",
    "        \"BD\": \"mean_squared_error\",\n",
    "        \"E\": \"mean_squared_error\",\n",
    "        \"FS\": \"mean_squared_error\",\n",
    "        \"A\": \"mean_squared_error\",\n",
    "        \"RC\": \"mean_squared_error\",\n",
    "    }\n",
    "    model.compile(#loss='mean_squared_error', # 'categorical_crossentropy' 'mean_squared_error' 'mean_absolute_percentage_error'\n",
    "                  loss=losses, \n",
    "              optimizer='adam') # 'adadelta' 'rmsprop'                  \n",
    "#     model.summary()\n",
    "    return model\n",
    "te=make_model_1d(0.001,30,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from IPython.display import SVG\n",
    "# from keras.utils import plot_model\n",
    "# from keras.utils.vis_utils import model_to_dot\n",
    "# SVG(model_to_dot(te).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# te.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_model(te, to_file='model1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customized data generator\n",
    "def generator_noise(data, label, batch_size=64,noise_range=0.1):\n",
    "    while 1:\n",
    "        DP = []  \n",
    "        BD = []\n",
    "        E = []\n",
    "        FS = []\n",
    "        A = []\n",
    "        RC = []\n",
    "        rows = np.random.randint(0, data.shape[0], size=batch_size)\n",
    "\n",
    "        samples = data[rows]\n",
    "        y=label[rows]\n",
    "        noise=np.random.uniform(low=-noise_range, high=noise_range, size=(y.shape[0],y.shape[1]))\n",
    "        y=y+noise\n",
    "        for i in range(len(y)):\n",
    "            DP.append(y[i][0])\n",
    "            BD.append(y[i][1])\n",
    "            E.append(y[i][2])\n",
    "            FS.append(y[i][3])\n",
    "            A.append(y[i][4])\n",
    "            RC.append(y[i][5])\n",
    "        labels = [np.array(DP),np.array(BD),np.array(E),np.array(FS), np.array(A),np.array(RC)]\n",
    "        yield samples,labels\n",
    "        \n",
    "def generator(data, label, batch_size=64):\n",
    "    while 1:\n",
    "        DP = []  \n",
    "        BD = []\n",
    "        E = []\n",
    "        FS = []\n",
    "        A = []\n",
    "        RC = []\n",
    "        rows = np.random.randint(0, data.shape[0], size=batch_size)\n",
    "\n",
    "        samples = data[rows]\n",
    "        y=label[rows]\n",
    "#         noise=np.random.uniform(low=-0.5, high=0.5, size=(y.shape[0],y.shape[1]))\n",
    "#         y=y+noise\n",
    "        for i in range(len(y)):\n",
    "            DP.append(y[i][0])\n",
    "            BD.append(y[i][1])\n",
    "            E.append(y[i][2])\n",
    "            FS.append(y[i][3])\n",
    "            A.append(y[i][4])\n",
    "            RC.append(y[i][5])\n",
    "        labels = [np.array(DP),np.array(BD),np.array(E),np.array(FS), np.array(A),np.array(RC)]\n",
    "        yield samples,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_hold_out(model_name,hold_out_number,E,show_result=False,l2lambd=0.001,noise_range=0.1,poolingweight=[0.2,0.4,0.4]):\n",
    "    # using leave one cv to get train/test data\n",
    "    X_train, X_test, y_train, y_test=hold_out(all_data,Y,hold_out_number)\n",
    "    \n",
    "    # scale all the value to 0-1\n",
    "    X_train=X_train/640-0.5\n",
    "    X_test=X_test/640-0.5\n",
    "    y_train=y_train/5\n",
    "    y_test=y_test/5\n",
    "#     y_train=y_train\n",
    "#     y_test=y_test\n",
    "    # print(X_train.shape,y_train.shape)\n",
    "    # print(X_test.shape,y_test.shape)\n",
    "\n",
    "    sample_of_trainningdata=X_train.shape[0]\n",
    "    sample_of_testdata=X_test.shape[0]\n",
    "\n",
    "    batch_size=128\n",
    "\n",
    "    train_gen=generator_noise(X_train, y_train, batch_size,noise_range)\n",
    "    val_gen=generator(X_test, y_test, batch_size)\n",
    "    \n",
    "    #prepare for inout shape\n",
    "    clip_lenth=X_train.shape[1]\n",
    "    dimension=X_train.shape[2]\n",
    "    \n",
    "    if model_name=='CNN':    \n",
    "        model=make_model_1d(l2lambd,clip_lenth,dimension)\n",
    "    if model_name =='CNN3':\n",
    "        model=make_model_1d_3layer(l2lambd,clip_lenth,dimension)\n",
    "        \n",
    "    model_h=model.fit_generator(train_gen,\n",
    "                        steps_per_epoch=sample_of_trainningdata//batch_size,\n",
    "                        epochs=E,\n",
    "                        validation_data=val_gen,\n",
    "                       validation_steps=sample_of_testdata//batch_size +1 ,\n",
    "                        verbose=0 )\n",
    "  \n",
    "    GT=list(getlabel(hold_out_number))\n",
    "\n",
    "    #get pridect result\n",
    "    predict=model.predict(X_test)\n",
    "    data={}\n",
    "    task=['DP','BD','E','FS','A','RC']\n",
    "\n",
    "    for i in range(len(predict)):\n",
    "        data[task[i]]=np.squeeze(predict[i])\n",
    "\n",
    "#         Plot_data=pd.DataFrame(predict,columns=['DP','BD','E','FS','A','RC'])\n",
    "    Plot_data=pd.DataFrame(data)\n",
    "    Plot_data=Plot_data*5\n",
    "\n",
    "#         print(Plot_data)\n",
    "#         print(Plot_data.shape)\n",
    "    video_result=get_video_score(Plot_data,GT,poolingweight)\n",
    "    gt=video_result.iloc[4]\n",
    "    re=video_result.iloc[3]\n",
    "    each_diff=np.abs(re-gt)\n",
    "    \n",
    "    if show_result:\n",
    "        ax=plt.figure(figsize=(12,14))\n",
    "        ax = plt.subplot(321)\n",
    "        ax.plot(model_h.history['loss'])\n",
    "        plt.ylabel('Training Loss')\n",
    "        plt.xlabel('epochs')\n",
    "\n",
    "        ax = plt.subplot(322)\n",
    "        ax.plot(model_h.history['val_loss'])\n",
    "        plt.ylabel('valadation Loss')\n",
    "        plt.xlabel('epochs')\n",
    "        print('loss',model_h.history['val_loss'][-1])\n",
    "    \n",
    "        y=[x for x in range(1,7)]\n",
    "        ax = plt.subplot(323)\n",
    "        Plot_data.plot.box(ax=ax)\n",
    "\n",
    "        \n",
    "        ax.plot(y, re, label='predict')\n",
    "        ax.plot(y, gt, label='ground truth')\n",
    "\n",
    "        plt.ylabel('Gear Score')\n",
    "        plt.xlabel('Score type')\n",
    "        plt.ylim(0,5.5)\n",
    "        ax.legend()\n",
    "\n",
    "        ax = plt.subplot(324)\n",
    "        Plot_data.plot(ax=ax)\n",
    "#         ax.plot(Plot_data)\n",
    "        ax.legend(['DP','BD','E','FS','A','RC'])\n",
    "#         ax.plot(Plot_data.BD)\n",
    "        plt.ylabel('Gear Score')\n",
    "        plt.xlabel('Video clips length')\n",
    "\n",
    "        \n",
    "        ax = plt.subplot(325)\n",
    "        times=get_review_times(hold_out_number)\n",
    "        ax.plot(y, np.round(re), label='predict-round')\n",
    "        ax.plot(y, re, label='predict')\n",
    "        \n",
    "        \n",
    "        plt.ylabel('Gear Score')\n",
    "        plt.xlabel('Score type')\n",
    "        plt.ylim(0,5.5)\n",
    "        \n",
    "        if times>1:\n",
    "#             ax.plot(y, re, label='predict')\n",
    "            \n",
    "            vmax,vmin=get_video_range(hold_out_number)\n",
    "#             print(vmax)\n",
    "#             print(vmin)\n",
    "            it=1\n",
    "            for types in ['DP','BD','E','FS','A','RC']:\n",
    "                Range=vmax[types]-vmin[types]\n",
    "                rect = Rectangle((it, vmin[types]), 0.2, Range, color='green')\n",
    "                ax.add_patch(rect)   \n",
    "                it=it+1\n",
    "        else:\n",
    "#             new_re=np.round(video_result.iloc[3])\n",
    "#             ax.plot(y, new_re, label='predict-round')\n",
    "            \n",
    "            ax.plot(y, gt, label='ground truth')            \n",
    "           \n",
    "        labels=['','DP','BD','E','FS','A','RC']\n",
    "        ax.set_xticklabels(labels)\n",
    "        ax.legend()\n",
    "#         plt.savefig(str(hold_out_number)+'result.png')\n",
    "        plt.show()\n",
    "        \n",
    "    return model_h.history['val_loss'][-1],list(each_diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 113, 130, 143, 152, 16, 164, 171, 176, 192, 194, 207, 236, 237, 240, 253, 257, 258, 49, 59, 74, 78, 91]\n"
     ]
    }
   ],
   "source": [
    "print(Y)\n",
    "# E=1\n",
    "# show_result=True\n",
    "# l2lam=0.000000001\n",
    "# noise_range=0.1\n",
    "\n",
    "# model_name='CNN'\n",
    "# print(\"you can try the video number above\")\n",
    "# l,d=test_hold_out(model_name,11,E,show_result,l2lam,noise_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ####un-comment the code blow to test all the video\n",
    "# # def test_hold_out(model_name,hold_out_number,E,show_result=False,l2=0.001,noise_range=0.1):\n",
    "# Diff_plot=[]\n",
    "# E=20\n",
    "# show_result=True\n",
    "# l2lam=0.01\n",
    "# noise_range=0.1\n",
    "\n",
    "# total_loss=0\n",
    "# model_name='CNN'\n",
    "# # for i in [11, 113, 130]:\n",
    "# for i in Y:\n",
    "#     print('video',i)\n",
    "#     loss,each_diff=test_hold_out(model_name,i,E,show_result,l2lam,noise_range,pooling)\n",
    "#     total_loss=total_loss+loss\n",
    "#     Diff_plot.append(each_diff)\n",
    "# print('total_loss= ',total_loss)\n",
    "\n",
    "# Diff_plot=np.array(Diff_plot)\n",
    "# Diff_plot=pd.DataFrame(Diff_plot)\n",
    "# Diff_plot.columns = ['DP','BD','E','FS','A','RC']\n",
    "# Diff_plot.plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_swarm_data(Diff_plot):\n",
    "    name=['DP','BD','E','FS','A','RC']\n",
    "    err=[]\n",
    "    typen=[]\n",
    "    for n in name:\n",
    "        for record in Diff_plot[n]:\n",
    "            err.append(record)\n",
    "            typen.append(n)\n",
    "\n",
    "    swarm_data = {'err': err, 'score_type':typen}\n",
    "    swarm_data = pd.DataFrame(data=swarm_data)\n",
    "    return swarm_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_slection(model_name,E,l2lam,noise_range,poolingweight):  \n",
    "    Diff_plot=[]\n",
    "#     E=20\n",
    "#     l2lam=0.01\n",
    "#     noise_range=0.1\n",
    "    show_result=False\n",
    "\n",
    "    total_loss=0\n",
    "#     for i in [11]:\n",
    "#     for i in [11, 113, 130, 143]:\n",
    "    for i in Y:\n",
    "        print('video',i,end=' ')\n",
    "        loss,each_diff=test_hold_out(model_name,i,E,show_result,l2lam,noise_range,poolingweight)\n",
    "        total_loss=total_loss+loss\n",
    "        Diff_plot.append(each_diff)\n",
    "\n",
    "    ax=plt.figure(figsize=(16,5))\n",
    "    \n",
    "    Diff_plot=np.array(Diff_plot)\n",
    "    Diff_plot=pd.DataFrame(Diff_plot)\n",
    "    Diff_plot.columns = ['DP','BD','E','FS','A','RC']\n",
    "    ax = plt.subplot(131)\n",
    "    Diff_plot.plot.box(ax=ax)\n",
    "    plt.ylim(0,4.5)\n",
    "    print('total_loss= ',total_loss)\n",
    "    \n",
    "\n",
    "    bx = plt.subplot(132)\n",
    "    swarm_data=get_swarm_data(Diff_plot)\n",
    "    bx = sns.swarmplot(x=\"score_type\", y=\"err\", data=swarm_data,color=\"0.2\")\n",
    "    ax = sns.boxplot(x=\"score_type\", y=\"err\", data=swarm_data)\n",
    "    plt.ylim(0,4.5)\n",
    "\n",
    "    \n",
    "    cx = plt.subplot(133)\n",
    "    cx = sns.violinplot(x=\"score_type\", y=\"err\", data=swarm_data)\n",
    "    plt.ylim(0,4.5)\n",
    "    plt.show()\n",
    "    \n",
    "    print('all gear score abs error',np.sum(np.sum(Diff_plot)))\n",
    "    print('each gear score abs error\\n',np.sum(Diff_plot))\n",
    "    weight=''\n",
    "    for i in poolingweight:\n",
    "        weight+=str(i)+'_'\n",
    "        \n",
    "    out_name='data//'+str(E)+'_'+str(l2lam)+'_'+str(noise_range)+'_'+str(weight)+'.npy'\n",
    "    np.save(out_name,Diff_plot)\n",
    "    \n",
    "    K.clear_session()\n",
    "    \n",
    "    tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################  Base Line ####################################\n",
    "Label_matrix=[]\n",
    "for num in Y:\n",
    "    Label_matrix.append(getlabel(num))\n",
    "Label_matrix=np.array(Label_matrix)\n",
    "sns.set(style=\"whitegrid\")\n",
    "fix=np.zeros((23,6))\n",
    "fix+=3\n",
    "ax=plt.figure(figsize=(16,5))\n",
    "diff=np.abs(fix-Label_matrix)\n",
    "ax = plt.subplot(131)\n",
    "Diff_plot=pd.DataFrame(diff)\n",
    "Diff_plot.columns = ['DP','BD','E','FS','A','RC']\n",
    "Diff_plot.plot.box(ax=ax)\n",
    "plt.ylim(0,4.5)\n",
    "\n",
    "ax = plt.subplot(132)\n",
    "swarm_data=get_swarm_data(Diff_plot)\n",
    "ax = sns.swarmplot(x=\"score_type\", y=\"err\", data=swarm_data,color=\"0.2\")\n",
    "ax = sns.boxplot(x=\"score_type\", y=\"err\", data=swarm_data)\n",
    "plt.ylim(0,4.5)\n",
    "\n",
    "ax = plt.subplot(133)\n",
    "swarm_data=get_swarm_data(Diff_plot)\n",
    "ax = sns.violinplot(x=\"score_type\", y=\"err\", data=swarm_data)\n",
    "plt.ylim(0,4.5)\n",
    "\n",
    "print(np.sum(np.sum(Diff_plot)))\n",
    "print(np.sum(Diff_plot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_slection('CNN',20,0.0001,0.1,[0,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_slection('CNN',20,0.0001,0.1,[1,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_slection('CNN',20,0.0001,0.1,[0,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_slection('CNN',20,0.0001,0.1,[0.333333,0.333333,0.333333])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_slection('CNN',10,0.0001,0.1,[0.333333,0.333333,0.333333])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_slection('CNN',10,0.0005,0.2,[0.333333,0.333333,0.333333])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_slection('CNN',10,0.0001,0.25,[0.333333,0.333333,0.333333])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_slection('CNN',20,0.00001,0.1,[0.333333,0.333333,0.333333])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_slection('CNN',10,0.00001,0.1,[0.333333,0.333333,0.333333])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_slection('CNN',10,0.00001,0.2,[0.333333,0.333333,0.333333])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_slection('CNN',10,0.00001,0.4,[0.333333,0.333333,0.333333])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_slection('CNN',20,0.001,0.1,[0.333333,0.333333,0.333333])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_slection('CNN',20,0.001,0.2,[0.333333,0.333333,0.333333])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_slection('CNN',20,0.001,0.5,[0.333333,0.333333,0.333333])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_slection('CNN3',20,0.001,0.2,[0.333333,0.333333,0.333333])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_slection('CNN3',20,0.001,0.1,[0.333333,0.333333,0.333333])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_slection('CNN',10,0.0001,0.1,[0.333333,0.333333,0.333333])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Diff_plot=[]\n",
    "show_result=True\n",
    "E=15\n",
    "l2lam=0.0001\n",
    "noise_range=0.1\n",
    "poolingweight=[0,1,0]\n",
    "\n",
    "total_loss=0\n",
    "model_name='CNN3'\n",
    "# for i in [11, 113, 130]:\n",
    "for i in Y:\n",
    "    print('video',i,end=' ')\n",
    "    loss,each_diff=test_hold_out(model_name,i,E,show_result,l2lam,noise_range,poolingweight)\n",
    "    total_loss=total_loss+loss\n",
    "    Diff_plot.append(each_diff)\n",
    "\n",
    "\n",
    "Diff_plot=np.array(Diff_plot)\n",
    "Diff_plot=pd.DataFrame(Diff_plot)\n",
    "Diff_plot.columns = ['DP','BD','E','FS','A','RC']\n",
    "Diff_plot.plot.box()\n",
    "plt.ylim(0,4.5)\n",
    "print('total_loss= ',total_loss)\n",
    "print('all 6 gear score error',np.sum(Diff_plot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GlobalMaxPooling1D, Falttern, GlobalMaxPooling1D+var  these three method did not have significant difference.<br>\n",
    "No regularizer + early stop is the current best method.<br>\n",
    "Latge regularizer makes smooth and average curve and the predict line tends to be stright "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
