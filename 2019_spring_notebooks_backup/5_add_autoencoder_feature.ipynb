{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D, Input,Flatten,Dense\n",
    "from keras.utils.data_utils import get_file\n",
    "import keras.backend as K\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.layers import Conv2D, Input\n",
    "import os\n",
    "from os.path import join as pjoin\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder-decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN_PIXEL = np.array([103.939, 116.779, 123.68])\n",
    "\n",
    "def vgg_layers(inputs, target_layer):\n",
    "    \n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same', name='block1_conv1')(inputs)\n",
    "#     x = Conv2D(32, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "\n",
    "    # Block 2\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "#     x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "\n",
    "    # Block 3\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "#     x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "#     x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "    x = Conv2D(1, (3, 3), activation='relu', padding='same', name='downsample')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def VGG19(input_tensor=None, input_shape=None, target_layer=1):\n",
    "    \"\"\"\n",
    "    VGG19, up to the target layer (1 for relu1_1, 2 for relu2_1, etc.)\n",
    "    \"\"\"\n",
    "    if input_tensor is None:\n",
    "        inputs = Input(shape=input_shape)\n",
    "    else:\n",
    "        inputs = Input(tensor=input_tensor, shape=input_shape)\n",
    "    model = Model(inputs, vgg_layers(inputs, target_layer), name='vgg19')\n",
    "    # load_weights(model)\n",
    "    return model\n",
    "\n",
    "\n",
    "def preprocess_input(x):\n",
    "    # Convert 'RGB' -> 'BGR'\n",
    "    if type(x) is np.ndarray:\n",
    "        x = x[..., ::-1]\n",
    "    else:\n",
    "        x = tf.reverse(x, [-1])\n",
    "\n",
    "    return x - MEAN_PIXEL\n",
    "\n",
    "\n",
    "\n",
    "from keras.layers import Input, Conv2D, UpSampling2D\n",
    "\n",
    "def decoder_layers(inputs, layer):\n",
    "\n",
    "    x=decoder_block4(inputs)\n",
    "    x=decoder_block3(x)\n",
    "    x=decoder_block2(x)\n",
    "    x=decoder_block1(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "def decoder_block1(inputs):\n",
    "    x = UpSampling2D((2, 2), name='decoder_block1_upsample')(inputs)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same', name='decoder_block1_conv2')(x)\n",
    "#     x = Conv2D(32, (3, 3), activation='relu', padding='same', name='decoder_block1_conv1')(x)\n",
    "    return x\n",
    "\n",
    "def decoder_block2(inputs):\n",
    "    x = UpSampling2D((2, 2), name='decoder_block2_upsample')(inputs)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='decoder_block2_conv2')(x)\n",
    "#     x = Conv2D(64, (3, 3), activation='relu', padding='same', name='decoder_block2_conv1')(x)\n",
    "    return x\n",
    "\n",
    "def decoder_block3(inputs):\n",
    "    x = UpSampling2D((2, 2), name='decoder_block3_upsample')(inputs)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='decoder_block3_conv3')(x)\n",
    "#     x = Conv2D(128, (3, 3), activation='relu', padding='same', name='decoder_block3_conv2')(x)\n",
    "    return x\n",
    "\n",
    "def decoder_block4(inputs):\n",
    "    x = UpSampling2D((2, 2), name='decoder_block4_upsample')(inputs)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='decoder_block4_conv3')(x)\n",
    "#     x = Conv2D(256, (3, 3), activation='relu', padding='same', name='decoder_block4_conv1')(x)\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.layers import Conv2D, Input\n",
    "import keras.backend as K\n",
    "\n",
    "LAMBDA=1\n",
    "\n",
    "def l2_loss(x):\n",
    "    return K.sum(K.square(x)) / 2\n",
    "\n",
    "class EncoderDecoder:\n",
    "    def __init__(self, input_shape=(256, 256, 3), target_layer=5,\n",
    "                 decoder_path=None):\n",
    "        self.input_shape = input_shape\n",
    "        self.target_layer = target_layer\n",
    "\n",
    "        self.encoder = VGG19(input_shape=input_shape, target_layer=target_layer)\n",
    "        if decoder_path:\n",
    "            self.decoder = load_model(decoder_path)\n",
    "        else:\n",
    "            self.decoder = self.create_decoder(target_layer)\n",
    "\n",
    "        self.model = Sequential()\n",
    "        self.model.add(self.encoder)\n",
    "        self.model.add(self.decoder)\n",
    "\n",
    "        self.loss = self.create_loss_fn(self.encoder)\n",
    "\n",
    "        self.model.compile('adam', self.loss)\n",
    "\n",
    "    def create_loss_fn(self, encoder):\n",
    "        def get_encodings(inputs):\n",
    "            encoder = VGG19(inputs, self.input_shape, self.target_layer)\n",
    "            return encoder.output\n",
    "\n",
    "        def loss(img_in, img_out):\n",
    "            encoding_in = get_encodings(img_in)\n",
    "            encoding_out = get_encodings(img_out)\n",
    "            return l2_loss(img_out - img_in) + \\\n",
    "                   LAMBDA*l2_loss(encoding_out - encoding_in)\n",
    "        return loss\n",
    "\n",
    "    def create_decoder(self, target_layer):\n",
    "        inputs = Input(shape=self.encoder.output_shape[1:])\n",
    "        layers = decoder_layers(inputs, target_layer)\n",
    "        output = Conv2D(3, (3, 3), activation='relu', padding='same',\n",
    "                        name='decoder_out')(layers)\n",
    "        return Model(inputs, output, name='decoder_%s' % target_layer)\n",
    "\n",
    "    def export_decoder(self):\n",
    "        self.decoder.save('decoder_%s.h5' % self.target_layer)\n",
    "\n",
    "    def export_encoder(self):\n",
    "        self.encoder.save('encoder_%s.h5' % self.target_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_decoder = EncoderDecoder(target_layer=3)\n",
    "encoder_decoder.encoder.load_weights('encoder_3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg19 (Model)                (None, 16, 16, 1)         390721    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "=================================================================\n",
      "Total params: 390,721\n",
      "Trainable params: 390,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Encoder=Sequential()\n",
    "Encoder.add(encoder_decoder.encoder)\n",
    "Encoder.add(Flatten())\n",
    "Encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input a image then pass it to the model to get reconstruction\n",
    "def get_reconstruction(model,img_path):\n",
    "    newimg=mpimg.imread( img_path)\n",
    "    #first we need to do the preprocessing\n",
    "    # resize to vgg input size\n",
    "    newimg=cv2.resize(newimg,(256,256))\n",
    "    # add the batch dimension\n",
    "    newimg = np.expand_dims(newimg, axis=0)\n",
    "    # get the reconstruction \n",
    "    reconstruction_m1=model.predict(newimg)\n",
    "\n",
    "    return reconstruction_m1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already saved features. No need to run code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now reading...... C:\\NewResizedData\\1  there are 739 images\n",
      "(739, 256)\n",
      "Now reading...... C:\\NewResizedData\\10  there are 968 images\n",
      "(968, 256)\n",
      "Now reading...... C:\\NewResizedData\\11  there are 753 images\n",
      "(753, 256)\n",
      "Now reading...... C:\\NewResizedData\\12  there are 731 images\n",
      "(731, 256)\n",
      "Now reading...... C:\\NewResizedData\\13  there are 640 images\n",
      "(640, 256)\n",
      "Now reading...... C:\\NewResizedData\\14  there are 731 images\n",
      "(731, 256)\n",
      "Now reading...... C:\\NewResizedData\\15  there are 486 images\n",
      "(486, 256)\n",
      "Now reading...... C:\\NewResizedData\\16  there are 737 images\n",
      "(737, 256)\n",
      "Now reading...... C:\\NewResizedData\\17  there are 735 images\n",
      "(735, 256)\n",
      "Now reading...... C:\\NewResizedData\\18  there are 734 images\n",
      "(734, 256)\n",
      "Now reading...... C:\\NewResizedData\\19  there are 678 images\n",
      "(678, 256)\n",
      "Now reading...... C:\\NewResizedData\\20  there are 615 images\n",
      "(615, 256)\n",
      "Now reading...... C:\\NewResizedData\\3  there are 808 images\n",
      "(808, 256)\n",
      "Now reading...... C:\\NewResizedData\\4  there are 739 images\n",
      "(739, 256)\n",
      "Now reading...... C:\\NewResizedData\\5  there are 742 images\n",
      "(742, 256)\n",
      "Now reading...... C:\\NewResizedData\\6  there are 733 images\n",
      "(733, 256)\n",
      "Now reading...... C:\\NewResizedData\\7  there are 668 images\n",
      "(668, 256)\n",
      "Now reading...... C:\\NewResizedData\\8  there are 745 images\n",
      "(745, 256)\n",
      "Now reading...... C:\\NewResizedData\\9  there are 754 images\n",
      "(754, 256)\n"
     ]
    }
   ],
   "source": [
    "# # get auto encoder feature for each video\n",
    "# data_dir='C:\\\\NewResizedData'\n",
    "# #data_dir contains 19 folders. Named as 1 2 3 4 5 .......\n",
    "# for img_dir in os.listdir(data_dir):\n",
    "#     # for each video\n",
    "#     each_data=[]   \n",
    "#     each_dir= pjoin(data_dir, img_dir)      #get each folder's name\n",
    "#     print('Now reading......',each_dir,' there are',len(os.listdir(each_dir)),'images')\n",
    "#     each_lable=img_dir\n",
    "#     # read image in the folder\n",
    "#     for i in range(1,len(os.listdir(each_dir))+1): \n",
    "        \n",
    "#         img = each_dir+'\\\\'+ str(each_lable)+'-'+str(i)+'.jpg'\n",
    "\n",
    "#         readimg=get_reconstruction(Encoder,img)# read each images and get feature\n",
    "#         readimg= np.squeeze(readimg)  \n",
    "\n",
    "#         each_data.append(readimg)\n",
    "        \n",
    "#     each_data=np.array(each_data)    \n",
    "#     print(each_data.shape)\n",
    "#     name='autoencoder_feature_'+ str(each_lable)+'.npy'\n",
    "#     np.save(name, each_data) # save auto encoder feature\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# first we can see what does the feature look like\n",
    "def get_raw_feature():\n",
    "    video_data_dir='C:\\\\videofeature\\\\'\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    for video in os.listdir(video_data_dir):\n",
    "        each_video= pjoin(video_data_dir,video)\n",
    "        l=each_video.split('t_')[1]\n",
    "        l=l.split('.')[0]\n",
    "        LABEL=int(l)\n",
    "        X.append(np.load(each_video))\n",
    "        Y.append(LABEL)\n",
    "    Video_label=pd.read_csv('label.csv')\n",
    "    Video_label=np.array(Video_label)[:,:-1]\n",
    "    return X,Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y=get_raw_feature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_data_dir='C:\\\\auto_feature\\\\'\n",
    "auto_X=[]\n",
    "for video in os.listdir(auto_data_dir):\n",
    "    each_video= pjoin(auto_data_dir,video)\n",
    "    auto_X.append(np.load(each_video))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finished saving combined_feature no need to run below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save combined feature\n",
    "# for x, x_auto,y in zip(X,auto_X,Y):\n",
    "#     combined_feature=np.hstack((x,x_auto))\n",
    "    \n",
    "#     name='combined_feature_'+ str(y)+'.npy'\n",
    "#     np.save(name, combined_feature) # save auto encoder feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
